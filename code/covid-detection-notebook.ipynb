{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AiApp Miniproject; Leonie DÃ¤ullary, Ruwen Frick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to categorize brain MRI images into four categories. The main goal is to train a nwtwork to detect alzeheimers disease in brain MRI images. The data can be found here: https://www.kaggle.com/datasets/sachinkumar413/alzheimer-mri-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opendatasets tensorflow matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "from keras import Sequential\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import Flatten, Dense, MaxPooling2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import opendatasets as od\n",
    "import os\n",
    "from pathlib import Path, PurePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dowloading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is hosted on Kaggle. When executing the following cell you will be asked for your Kaggle credentials. These can be acquired by following below steps:\n",
    "\n",
    "1. Sign in to https://kaggle.com/ or register a new account, then click on your profile picture on the top right and select \"My Account\" from the menu.\n",
    "\n",
    "2. Scroll down to the \"API\" section and click \"Create New API Token\". This will download a file kaggle.json with the following contents:\n",
    "    {\"username\":\"YOUR_KAGGLE_USERNAME\",\"key\":\"YOUR_KAGGLE_KEY\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://www.kaggle.com/sachinkumar413/alzheimer-mri-dataset'\n",
    "od.download(dataset_url)\n",
    "\n",
    "path = Path(\".\", 'alzheimer-mri-dataset', 'Dataset').absolute()\n",
    "print(f\"Data stored at: {path}\")\n",
    "\n",
    "classes = [dir for dir in sorted(os.listdir(path))]\n",
    "\n",
    "mild_path = Path(path, classes[0])\n",
    "moderate_path = Path(path, classes[1])\n",
    "non_path = Path(path, classes[2])\n",
    "verymild_path = Path(path, classes[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print metadata of downloaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(path):\n",
    "    class_name = PurePath(dirpath).name\n",
    "    if class_name != 'Dataset':\n",
    "        print(f'{len(filenames)} images in class {class_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs = []\n",
    "\n",
    "for i in range(2, 5):\n",
    "    mild_img = mpimg.imread(Path(mild_path, 'mild_' + str(i) + '.jpg'))\n",
    "    all_imgs.append(mild_img)\n",
    "\n",
    "    moderate_img = mpimg.imread(Path(moderate_path, 'moderate_' + str(i) + '.jpg'))\n",
    "    all_imgs.append(moderate_img)\n",
    "\n",
    "    non_img = mpimg.imread(Path(non_path, 'non_' + str(i) + '.jpg'))\n",
    "    all_imgs.append(non_img)\n",
    "\n",
    "    verymild_img = mpimg.imread(Path(verymild_path, 'verymild_' + str(i) + '.jpg'))\n",
    "    all_imgs.append(verymild_img)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for index in range(1, len(all_imgs) + 1):\n",
    "    plt.subplot(3, 4, index)\n",
    "    plt.imshow(all_imgs[index - 1])\n",
    "    plt.title(classes[(index - 1) % len(classes)])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation and augmentation parameters\n",
    "image_size = (128, 128)\n",
    "horizontal_flip = True\n",
    "color_mode = 'rgb'\n",
    "zoom_range = 0.05\n",
    "rotation_range = 10\n",
    "shear_range = 0.1\n",
    "batch_size  = 64\n",
    "validation_split = 0.15\n",
    "\n",
    "train_data_generator = ImageDataGenerator(\n",
    "    horizontal_flip = horizontal_flip,\n",
    "    zoom_range = zoom_range,\n",
    "    rotation_range = rotation_range,\n",
    "    shear_range = shear_range,\n",
    "    validation_split = validation_split\n",
    ")\n",
    "\n",
    "valid_data_generator = ImageDataGenerator(\n",
    "    validation_split = validation_split\n",
    ")\n",
    "\n",
    "train_data = train_data_generator.flow_from_directory(\n",
    "    path,\n",
    "    target_size = image_size,\n",
    "    color_mode = color_mode,\n",
    "    batch_size  = batch_size,\n",
    "    subset = 'training'\n",
    ")\n",
    "\n",
    "valid_data = valid_data_generator.flow_from_directory(\n",
    "    path,\n",
    "    target_size = image_size,\n",
    "    color_mode = color_mode,\n",
    "    batch_size  = batch_size,\n",
    "    subset = 'validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model = Sequential([\n",
    "  Conv2D(64, (4, 4), activation = 'relu', input_shape = (128, 128, 3)),\n",
    "  MaxPooling2D((3, 3)),\n",
    "  Conv2D(32, (3, 3), activation = 'relu'),\n",
    "  MaxPooling2D((2, 2)),\n",
    "  Conv2D(32, (2, 2), activation = 'relu'),\n",
    "  MaxPooling2D((2, 2)),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(len(classes), activation='softmax'),\n",
    "])\n",
    "\n",
    "shallow_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(classes), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile, fit and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model.compile(optimizer = 'adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "shallow_model.compile(optimizer = 'adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_history = deep_model.fit(\n",
    "    train_data,\n",
    "    epochs = 5,\n",
    "    validation_data = valid_data,\n",
    ")\n",
    "\n",
    "deep_acc = deep_model.evaluate(valid_data)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_history = shallow_model.fit(\n",
    "    train_data,\n",
    "    epochs = 5,\n",
    "    validation_data = valid_data,\n",
    ")\n",
    "\n",
    "shallow_acc = deep_model.evaluate(valid_data)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print accuracy\n",
    "print(f'Validation accuracy of deep model: {deep_acc}')\n",
    "print(f'Validation accuracy of shallow model: {shallow_acc}')\n",
    "\n",
    "\n",
    "# plot accuracy against epochs\n",
    "plt.title('Deep Model vs Shallow Model')\n",
    "plt.plot(deep_history.history['accuracy'], label= 'deep_accuracy')\n",
    "plt.plot(deep_history.history['val_accuracy'], label = 'deep_val_accuracy')\n",
    "plt.plot(shallow_history.history['accuracy'], label= 'shallow_accuracy')\n",
    "plt.plot(shallow_history.history['val_accuracy'], label = 'shallow_val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 0.8)\n",
    "plt.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance overall is not great. Validation accuracy being below training accuracy might indicate some overfitting\n",
    "# TO DO: discuss differences"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89a3b88996f9566b80268bdb916a4570dd89e84dc7968f80e8501a1bc5d1e00f"
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
